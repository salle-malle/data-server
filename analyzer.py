import json, logging, re, time
from datetime import datetime
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
import tiktoken

# 1. ê³µí†µ ì„¤ì • -------------------------------------------------------------
load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# max_tokens ì¦ê°€ë¡œ ë” ìƒì„¸í•œ ìš”ì•½ ìƒì„±
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=1200)
enc = tiktoken.encoding_for_model("gpt-4o-mini")

# ğŸ“Œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ - íˆ¬ìì ê´€ì ê³¼ ìƒì„¸ ë§¥ë½ ì¶”ê°€
prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ SEC 8-K ê³µì‹œë¥¼ ë¶„ì„í•˜ëŠ” **ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸**ì…ë‹ˆë‹¤. 
íˆ¬ììì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ë¬¸ê°€ê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ **ë§¥ë½ê³¼ í•¨ì˜ë¥¼ í¬í•¨í•œ ìƒì„¸ ë¶„ì„**ì„ ì œê³µí•˜ì„¸ìš”.

ê³µì‹œì¼: {filing_date}
ë¬¸ì„œ ë‚´ìš©: {full_content}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:

{{
  "title": "ê³µì‹œ í•µì‹¬ ë‚´ìš©ì„ ë°˜ì˜í•œ êµ¬ì²´ì ì¸ í•œêµ­ì–´ ì œëª© (50ì ì´ë‚´)",
  "narrative": "ê³µì‹œ ë‚´ìš©ì„ ìŠ¤í† ë¦¬í…”ë§ ë°©ì‹ìœ¼ë¡œ ì„¤ëª…í•œ 2-3ê°œ ë‹¨ë½ (ê° ë‹¨ë½ 4-5ë¬¸ì¥, ì—°ê²°ì–´ ì‚¬ìš©ìœ¼ë¡œ íë¦„ ìœ ì§€)",
  "investor_insights": [
    "íˆ¬ìì ê´€ì ì—ì„œ ì£¼ëª©í•  ì  3ê°€ì§€ (ê° 25ì ì´ë‚´)",
    "ì‹¤ì œ íˆ¬ì ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” êµ¬ì²´ì  ì •ë³´"
  ],
  "financial_impact": {{
    "impact_type": "positive/neutral/negative",
    "description": "ì¬ë¬´ì  ì˜í–¥ì— ëŒ€í•œ êµ¬ì²´ì  ì„¤ëª… (1-2ë¬¸ì¥)",
    "timeline": "ì˜í–¥ì´ ë‚˜íƒ€ë‚  ì˜ˆìƒ ì‹œê¸°"
  }},
  "key_figures": [
    "ë¬¸ì„œì— ì–¸ê¸‰ëœ ì¤‘ìš” ìˆ˜ì¹˜ë‚˜ ë‚ ì§œ ì •ë³´"
  ],
  "main_events": [
    {{
      "event_type": "ì‚¬ê±´ ìœ í˜•",
      "description": "ì‚¬ê±´ì˜ ë°°ê²½ê³¼ ì˜ë¯¸ë¥¼ í¬í•¨í•œ ìƒì„¸ ì„¤ëª…",
      "business_impact": "ë¹„ì¦ˆë‹ˆìŠ¤ì— ë¯¸ì¹˜ëŠ” ì˜í–¥",
      "importance": 1-10
    }}
  ],
  "tone": "positive/neutral/negative",
  "filing_date": "{filing_date}"
}}

ì¤‘ìš” ì§€ì¹¨:
- narrativeëŠ” **ì—°ê²°ì–´ì™€ ë§¥ë½**ì„ ì‚¬ìš©í•´ ìŠ¤í† ë¦¬ì²˜ëŸ¼ ì„œìˆ 
- investor_insightsëŠ” **ì‹¤ìš©ì ì´ê³  í–‰ë™ ì§€í–¥ì **ìœ¼ë¡œ ì‘ì„±
- ë‹¨ìˆœ ë²ˆì—­ì´ ì•„ë‹Œ **ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ê³¼ í•¨ì˜** í¬í•¨
- ìˆ˜ì¹˜ë‚˜ ë‚ ì§œëŠ” ì •í™•íˆ ì¸ìš©, ì¶”ì¸¡ ê¸ˆì§€
- ëª¨ë“  ë‚´ìš©ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±
""")

# 2. í•µì‹¬ í‚¤ì›Œë“œ ì„¸íŠ¸ - í™•ì¥ ë° ê°€ì¤‘ì¹˜ ì¶”ê°€ ------------------------------
CRITICAL_KEYWORDS = {
    "merger_acquisition": {
        "keywords": ["merger", "acquisition", "agreement", "definitive", "purchase", "sale", "disposition", "divestiture"],
        "weight": 1.5
    },
    "executive_changes": {
        "keywords": ["resign", "appointment", "terminate", "ceo", "cfo", "director", "president", "officer", "retire"],
        "weight": 1.3
    },
    "financial_events": {
        "keywords": ["earnings", "revenue", "loss", "dividend", "bankruptcy", "impairment", "writedown", "restructuring"],
        "weight": 1.4
    },
    "material_agreements": {
        "keywords": ["material definitive agreement", "joint venture", "partnership", "contract", "license"],
        "weight": 1.2
    },
    "legal_regulatory": {
        "keywords": ["lawsuit", "settlement", "regulatory", "compliance", "investigation", "sec", "doj"],
        "weight": 1.1
    }
}

# ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
ALL_KEYWORDS = []
for category, info in CRITICAL_KEYWORDS.items():
    for keyword in info["keywords"]:
        ALL_KEYWORDS.append((keyword.lower(), info["weight"]))

# ê¸¸ì´ ìˆœìœ¼ë¡œ ì •ë ¬ (ê¸´ í‚¤ì›Œë“œê°€ ë¨¼ì € ë§¤ì¹­ë˜ë„ë¡)
ALL_KEYWORDS.sort(key=lambda x: len(x[0]), reverse=True)

# 3. ìœ í‹¸ í•¨ìˆ˜ë“¤ ----------------------------------------------------------
def build_flexible_item_pattern(major: int, minor: int) -> str:
    return (
        rf"(?i)item[\s.\xa0]*{major}[\s.\xa0]*[\.:,]?\s*{minor}"
        r".*?(?=item[\s.\xa0]*\d+[\s.\xa0]*[\.:,]?\s*\d+|signature|$)"
    )

def extract_financial_numbers(text: str) -> list[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì¬ë¬´ ê´€ë ¨ ìˆ˜ì¹˜ ì¶”ì¶œ"""
    patterns = [
        r'\$[\d,]+(?:\.\d{2})?(?:\s*(?:million|billion|trillion|ë°±ë§Œ|ì–µ|ì¡°))?',
        r'[\d,]+(?:\.\d{2})?\s*(?:million|billion|trillion|ë°±ë§Œ|ì–µ|ì¡°)\s*(?:ë‹¬ëŸ¬|dollars?)',
        r'[\d,]+(?:\.\d{2})?\s*(?:shares?|ì£¼ì‹|ì£¼)',
        r'[\d,]+(?:\.\d{2})?\s*(?:percent|%)',
    ]
    
    numbers = []
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        numbers.extend(matches)
    
    return list(set(numbers))  # ì¤‘ë³µ ì œê±°

# 4. ê°œì„ ëœ Smart8KExtractor -----------------------------------------------
class Smart8KExtractor:
    def __init__(self):
        self.item_patterns = {
            "ITEM_1_01": {
                "pattern": build_flexible_item_pattern(1, 1),
                "importance": 8,
                "korean_name": "ì¤‘ìš” ê³„ì•½ ì²´ê²°",
                "keywords": ["agreement", "contract", "definitive", "material"]
            },
            "ITEM_1_02": {
                "pattern": build_flexible_item_pattern(1, 2),
                "importance": 7,
                "korean_name": "ê³„ì•½ ì¢…ë£Œ",
                "keywords": ["termination", "terminate", "end", "expire"]
            },
            "ITEM_2_01": {
                "pattern": build_flexible_item_pattern(2, 1),
                "importance": 9,
                "korean_name": "ìì‚° ì¸ìˆ˜/ë§¤ê°",
                "keywords": ["acquisition", "merger", "purchase", "sale", "dispose", "acquire"]
            },
            "ITEM_2_02": {
                "pattern": build_flexible_item_pattern(2, 2),
                "importance": 8,
                "korean_name": "ì‹¤ì  ë°œí‘œ",
                "keywords": ["earnings", "revenue", "results", "financial", "quarter", "fiscal"]
            },
            "ITEM_3_02": {
                "pattern": build_flexible_item_pattern(3, 2),
                "importance": 6,
                "korean_name": "ì£¼ì‹ ë§¤ê°",
                "keywords": ["sale", "equity", "stock", "shares"]
            },
            "ITEM_4_01": {
                "pattern": build_flexible_item_pattern(4, 1),
                "importance": 6,
                "korean_name": "ê°ì‚¬ì¸ ë³€ê²½",
                "keywords": ["auditor", "accountant", "change"]
            },
            "ITEM_5_02": {
                "pattern": build_flexible_item_pattern(5, 2),
                "importance": 9,
                "korean_name": "ì„ì› ë³€ê²½",
                "keywords": ["appoint", "resign", "retire", "ceo", "cfo", "president", "officer", "director"]
            },
            "ITEM_7_01": {
                "pattern": build_flexible_item_pattern(7, 1),
                "importance": 7,
                "korean_name": "ê·œì œ ì ˆì°¨",
                "keywords": ["regulatory", "proceeding", "investigation"]
            },
            "ITEM_8_01": {
                "pattern": build_flexible_item_pattern(8, 1),
                "importance": 5,
                "korean_name": "ê¸°íƒ€ ì¤‘ìš” ì‚¬ê±´",
                "keywords": ["other", "event", "material"]
            },
            "ITEM_9_01": {
                "pattern": build_flexible_item_pattern(9, 1),
                "importance": 4,
                "korean_name": "ì¬ë¬´ì œí‘œ ë° ì „ì‹œë¬¼",
                "keywords": ["financial", "statements", "exhibits"]
            }
        }

        self.date_patterns = [
            r"(?i)date\s+of\s+report[:\(]?\s*(\w+\s+\d{1,2},?\s+\d{4})",
            r"(?i)filing\s+date[:\(]?\s*(\w+\s+\d{1,2},?\s+\d{4})",
            r"\((\w+\s+\d{1,2},?\s+\d{4})\)",
            r"(\d{4}-\d{2}-\d{2})",
            r"(\d{1,2}/\d{1,2}/\d{4})",
            r"(?i)(?:on|dated?|as of)\s+(\w+\s+\d{1,2},?\s+\d{4})"
        ]

    def smart_filter(self, raw_html: str) -> dict:
        soup = BeautifulSoup(raw_html, "html.parser")
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup.find_all(["header", "footer", "nav", "script", "style"]):
            tag.decompose()
        
        full_text = soup.get_text(separator=" ")
        
        # ğŸ“Œ í…Œì´ë¸” ì •ë³´ - ë” í¬ê´„ì ìœ¼ë¡œ ìˆ˜ì§‘
        tables = []
        for tbl in soup.find_all("table"):
            txt = tbl.get_text(" ", strip=True)
            # ë” ë§ì€ ì¤‘ìš” í‚¤ì›Œë“œ í¬í•¨
            if any(kw in txt.lower() for kw in [
                "merger", "agreement", "officer", "director", "shares", "vote", 
                "financial", "revenue", "earnings", "dividend", "debt"
            ]):
                tables.append(txt)
        
        # ê°•ì¡°ëœ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        emphasizes = []
        for em in soup.find_all(["b", "strong", "em", "u"]):
            txt = em.get_text(" ", strip=True)
            if len(txt) > 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                emphasizes.append(txt)
        
        return {
            "full_text": full_text,
            "tables": tables[:3],  # 3ê°œë¡œ ì¦ê°€
            "emphasized": emphasizes[:15]  # 15ê°œë¡œ ì¦ê°€
        }

    def extract_filing_date(self, raw_html: str) -> str:
        plain = BeautifulSoup(raw_html, "html.parser").get_text(" ")
        head = plain[:2000]
        
        for p in self.date_patterns:
            m = re.findall(p, head)
            if m:
                if date := self.normalize_date(m[0]):
                    return date
        
        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¬ê²€ìƒ‰
        for p in self.date_patterns:
            m = re.findall(p, plain)
            if m:
                if date := self.normalize_date(m[0]):
                    return date
        
        return "ë‚ ì§œ ì •ë³´ ì—†ìŒ"

    def normalize_date(self, s: str) -> str | None:
        fmts = ["%B %d, %Y", "%b %d, %Y", "%Y-%m-%d", "%m/%d/%Y", "%m-%d-%Y"]
        for f in fmts:
            try:
                return datetime.strptime(s.strip(), f).strftime("%Y-%m-%d")
            except ValueError:
                continue
        
        if s.isdigit() and len(s) == 8:
            return f"{s[:4]}-{s[4:6]}-{s[6:]}"
        
        return None

    def extract_important_content(self, text: str) -> str:
        """ğŸ“Œ ë” í’ë¶€í•œ ë§¥ë½ ì •ë³´ë¥¼ ì¶”ì¶œ"""
        important_parts = []
        
        # 1) Item íŒ¨í„´ ë§¤ì¹­ - ë” ê¸´ ë¸”ë¡ í—ˆìš©
        for code, cfg in sorted(
            self.item_patterns.items(),
            key=lambda x: x[1]["importance"],
            reverse=True
        ):
            matches = re.findall(cfg["pattern"], text, re.DOTALL)
            if matches:
                block = matches[0].strip()
                if len(block) > 80:  # ìµœì†Œ ê¸¸ì´ 80ìë¡œ ì¦ê°€
                    # ë¸”ë¡ í¬ê¸°ë¥¼ 1500ìë¡œ ì¦ê°€
                    important_parts.append(f"[{cfg['korean_name']}]\n{block[:1500]}")
        
        # 2) í‚¤ì›Œë“œ ê¸°ë°˜ ì¤‘ìš” ë¬¸ì¥ - ê°€ì¤‘ì¹˜ ì ìš©
        sentences = re.split(r"[.!?]+", text)
        weighted_sentences = []
        
        for sent in sentences:
            sent = sent.strip()
            if len(sent) < 40:  # ìµœì†Œ ê¸¸ì´ 40ìë¡œ ê°ì†Œ
                continue
            
            # ê°€ì¤‘ì¹˜ ê³„ì‚°
            total_weight = 0
            for keyword, weight in ALL_KEYWORDS:
                if keyword in sent.lower():
                    total_weight += weight
            
            if total_weight >= 1.5:  # ê°€ì¤‘ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
                weighted_sentences.append((sent, total_weight))
        
        # ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ë¬¸ì¥ ì„ íƒ
        weighted_sentences.sort(key=lambda x: x[1], reverse=True)
        for sent, weight in weighted_sentences[:12]:  # 12ê°œë¡œ ì¦ê°€
            important_parts.append(sent)
        
        # 3) ìˆ˜ì¹˜ ì •ë³´ ì¶”ê°€
        financial_numbers = extract_financial_numbers(text)
        if financial_numbers:
            numbers_text = "ì£¼ìš” ìˆ˜ì¹˜: " + ", ".join(financial_numbers[:10])
            important_parts.append(numbers_text)
        
        # 4) í†µí•© ë° í† í° ì œí•œ
        combined = "\n\n".join(important_parts[:15])  # 15ê°œ ì„¹ì…˜ìœ¼ë¡œ ì¦ê°€
        
        # ğŸ“Œ í† í° ìˆ˜ ì œí•œì„ 4500ìœ¼ë¡œ ì¦ê°€
        if len(enc.encode(combined)) > 4500:
            combined = combined[:12000]  # ë¬¸ì ìˆ˜ ì œí•œë„ ì¦ê°€
        
        return combined

# 5. ê°œì„ ëœ ë¶„ì„ í•¨ìˆ˜ ------------------------------------------------------
def analyze_8k(docs: list[str], max_cost: float = 8.0) -> list[dict]:  # ì˜ˆì‚° ì¦ê°€
    if not docs:
        return []

    extractor = Smart8KExtractor()
    results, total_cost = [], 0.0

    for idx, raw in enumerate(docs):
        logger.info("ë¬¸ì„œ %s ë¶„ì„ ì¤‘ (ê°œì„ ëœ ë²„ì „)...", idx + 1)
        
        # í† í° ë¹„ìš© ì¬ê³„ì‚° (ë” ë§ì€ í† í° ì‚¬ìš©)
        est_tokens = len(enc.encode(raw[:15_000])) * 0.4  # ì¶”ì • ë¹„ìœ¨ ì¦ê°€
        est_dollars = est_tokens / 1_000 * 0.00075
        if total_cost + est_dollars > max_cost:
            logger.warning("ì˜ˆì‚° ì´ˆê³¼ ì˜ˆìƒ, ë¬¸ì„œ %s ìŠ¤í‚µ", idx + 1)
            continue

        filing_date = extractor.extract_filing_date(raw)
        filtered = extractor.smart_filter(raw)
        
        # ğŸ“Œ ë” í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        important_content = extractor.extract_important_content(filtered["full_text"])
        
        # í…Œì´ë¸” ì •ë³´ ì¶”ê°€ (ë” ë§ì€ ì •ë³´)
        if filtered["tables"]:
            table_content = "\n".join(filtered["tables"])
            important_content += f"\n\n[í…Œì´ë¸” ì •ë³´]\n{table_content[:1500]}"  # 1500ìë¡œ ì¦ê°€
        
        # ê°•ì¡°ëœ í…ìŠ¤íŠ¸ ì¶”ê°€
        if filtered["emphasized"]:
            emphasized_content = "\n".join(filtered["emphasized"][:10])
            important_content += f"\n\n[ê°•ì¡°ëœ ë‚´ìš©]\n{emphasized_content}"

        # API í˜¸ì¶œ ì „ ë”œë ˆì´
        if idx > 0:
            logger.info("API Rate Limit ë°©ì§€ë¥¼ ìœ„í•´ 3ì´ˆ ëŒ€ê¸°...")
            time.sleep(3)  # 3ì´ˆë¡œ ì¦ê°€

        # 429 ì˜¤ë¥˜ ì¬ì‹œë„ ë¡œì§
        max_retries = 5
        retry_delay = 2  # ì´ˆê¸° ë”œë ˆì´ 2ì´ˆë¡œ ì¦ê°€
        
        for attempt in range(max_retries):
            try:
                response = (prompt | llm | StrOutputParser()).invoke({
                    "filing_date": filing_date,
                    "full_content": important_content
                })
                break
                
            except Exception as e:
                error_msg = str(e)
                if "429" in error_msg or "rate" in error_msg.lower():
                    logger.warning(
                        "429 ì˜¤ë¥˜ ë°œìƒ. %sì´ˆ í›„ ì¬ì‹œë„ (%d/%d)",
                        retry_delay, attempt + 1, max_retries
                    )
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    if attempt == max_retries - 1:
                        logger.error("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë¬¸ì„œ %s ìŠ¤í‚µ", idx + 1)
                        response = None
                else:
                    logger.error("API í˜¸ì¶œ ì‹¤íŒ¨: %s", e)
                    response = None
                    break

        if response is None:
            continue

        try:
            record = json.loads(response)
            
            # ğŸ“Œ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ë³´ê°•
            record.update({
                "document_index": idx + 1,
                "filing_date": filing_date,
                "processed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "analysis_version": "v4_enhanced",  # ë²„ì „ ì •ë³´ ì¶”ê°€
                "content_length": len(important_content)
            })
            
            # ë¹„ìš© ê³„ì‚°
            token_in = len(enc.encode(important_content))
            token_out = len(enc.encode(response))
            cost = token_in * 0.00015 / 1_000 + token_out * 0.00060 / 1_000
            total_cost += cost
            
            results.append(record)
            logger.info("ë¬¸ì„œ %s ë¶„ì„ ì™„ë£Œ (ë¹„ìš© +$%.4f, í† í°: %d)", idx + 1, cost, token_in)
            
        except json.JSONDecodeError as e:
            logger.error("JSON íŒŒì‹± ì‹¤íŒ¨ - ë¬¸ì„œ %s: %s", idx + 1, e)
            continue

        if total_cost >= max_cost:
            logger.info("ì˜ˆì‚° ì†Œì§„, ì¶”ê°€ ë¬¸ì„œ ì¤‘ë‹¨")
            break

    logger.info("ë¶„ì„ ì™„ë£Œ: %sê±´, ì´ ë¹„ìš© $%.4f", len(results), total_cost)

    # ğŸ“Œ ê°œì„ ëœ ë¹ˆ ê²°ê³¼ ì²˜ë¦¬
    if not results:
        return [{
            "title": "ë¶„ì„ ê°€ëŠ¥í•œ ì¤‘ìš” ê³µì‹œ ë‚´ìš© ì—†ìŒ",
            "narrative": "ì œê³µëœ ë¬¸ì„œì—ì„œ íˆ¬ììë‚˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ í˜•ì‹ì´ë‚˜ ë‚´ìš©ì— ë¬¸ì œê°€ ìˆê±°ë‚˜, ì¤‘ìš”ë„ê°€ ë‚®ì€ ê¸°ìˆ ì  ê³µì‹œì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.",
            "investor_insights": [
                "ì¶”ê°€ ì •ë³´ í•„ìš”",
                "ë‹¤ë¥¸ ê³µì‹œ ë¬¸ì„œ ê²€í†  ê¶Œì¥",
                "ì›ë¬¸ ì§ì ‘ í™•ì¸ í•„ìš”"
            ],
            "financial_impact": {
                "impact_type": "neutral",
                "description": "ëª…í™•í•œ ì¬ë¬´ì  ì˜í–¥ ì—†ìŒ",
                "timeline": "í•´ë‹¹ ì—†ìŒ"
            },
            "key_figures": [],
            "main_events": [],
            "tone": "neutral",
            "filing_date": "ë‚ ì§œ ì •ë³´ ì—†ìŒ",
            "document_index": 0,
            "analysis_version": "v4_enhanced"
        }]

    return results
