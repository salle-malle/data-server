"""
analyzer.py

SEC 8-K ë¬¸ì„œë¥¼ ì•ˆì •ì ìœ¼ë¡œ íŒŒì‹±Â·ìš”ì•½í•˜ê¸° ìœ„í•œ ë¶„ì„ê¸°
- BeautifulSoup ParserRejectedMarkup ì˜¤ë¥˜ ë°©ì§€(ì œì–´ë¬¸ ì œê±° + ë‹¤ì¤‘ íŒŒì„œ í´ë°±)
- gzipâ€†Â·â€†ì˜ëª»ëœ ì¸ì½”ë”© ìë™ ì²˜ë¦¬
- LangChain-OpenAIë¥¼ ì´ìš©í•´ í•œêµ­ì–´ ìš”ì•½(JSON í¬ë§·) ìƒì„±
"""

import json
import logging
import re
import time
import gzip
from datetime import datetime, date

import chardet
from bs4 import BeautifulSoup, UnicodeDammit
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
import tiktoken

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ê³µí†µ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=1200)
enc = tiktoken.encoding_for_model("gpt-4o-mini")

PROMPT = ChatPromptTemplate.from_template(
    """
ë‹¹ì‹ ì€ SEC 8-K ê³µì‹œë¥¼ ë¶„ì„í•˜ëŠ” **ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸**ì…ë‹ˆë‹¤. 
íˆ¬ììì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ë¬¸ê°€ê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ **ë§¥ë½ê³¼ í•¨ì˜ë¥¼ í¬í•¨í•œ ìƒì„¸ ë¶„ì„**ì„ ì œê³µí•˜ì„¸ìš”.

ê³µì‹œì¼: {filing_date}
ë¬¸ì„œ ë‚´ìš©: {full_content}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:

{{
  "title": "ê³µì‹œ í•µì‹¬ ë‚´ìš©ì„ ë°˜ì˜í•œ êµ¬ì²´ì ì¸ í•œêµ­ì–´ ì œëª© (50ì ì´ë‚´)",
  "narrative": "ê³µì‹œ ë‚´ìš©ì„ ìŠ¤í† ë¦¬í…”ë§ ë°©ì‹ìœ¼ë¡œ ì„¤ëª…í•œ 2-3ê°œ ë‹¨ë½ (ê° ë‹¨ë½ 4-5ë¬¸ì¥, ì—°ê²°ì–´ ì‚¬ìš©ìœ¼ë¡œ íë¦„ ìœ ì§€)",
  "filing_date": "{filing_date}"
}}

ì¤‘ìš” ì§€ì¹¨:
- narrativeëŠ” **ì—°ê²°ì–´ì™€ ë§¥ë½**ì„ ì‚¬ìš©í•´ ìŠ¤í† ë¦¬ì²˜ëŸ¼ ì„œìˆ 
- ë‹¨ìˆœ ë²ˆì—­ì´ ì•„ë‹Œ **ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ê³¼ í•¨ì˜** í¬í•¨
- ìˆ˜ì¹˜ë‚˜ ë‚ ì§œëŠ” ì •í™•íˆ ì¸ìš©, ì¶”ì¸¡ ê¸ˆì§€
- ëª¨ë“  ë‚´ìš©ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±
"""
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. í‚¤ì›Œë“œ ë° ì •ê·œì‹ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CRITICAL_KEYWORDS = {
    "merger_acquisition": {
        "keywords": [
            "merger",
            "acquisition",
            "agreement",
            "definitive",
            "purchase",
            "sale",
            "disposition",
            "divestiture",
        ],
        "weight": 1.5,
    },
    "executive_changes": {
        "keywords": [
            "resign",
            "appointment",
            "terminate",
            "ceo",
            "cfo",
            "director",
            "president",
            "officer",
            "retire",
        ],
        "weight": 1.3,
    },
    "financial_events": {
        "keywords": [
            "earnings",
            "revenue",
            "loss",
            "dividend",
            "bankruptcy",
            "impairment",
            "writedown",
            "restructuring",
        ],
        "weight": 1.4,
    },
    "material_agreements": {
        "keywords": [
            "material definitive agreement",
            "joint venture",
            "partnership",
            "contract",
            "license",
        ],
        "weight": 1.2,
    },
    "legal_regulatory": {
        "keywords": [
            "lawsuit",
            "settlement",
            "regulatory",
            "compliance",
            "investigation",
            "sec",
            "doj",
        ],
        "weight": 1.1,
    },
}

ALL_KEYWORDS = [
    (kw.lower(), info["weight"]) for info in CRITICAL_KEYWORDS.values() for kw in info["keywords"]
]
ALL_KEYWORDS.sort(key=lambda x: len(x[0]), reverse=True)  # ê¸´ í‚¤ì›Œë“œ ìš°ì„ 

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. í—¬í¼ / ìœ í‹¸ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_item_pattern(major: int, minor: int) -> str:
    return (
        rf"(?i)item[\s.\xa0]*{major}[\s.\xa0]*[\.:,]?\s*{minor}"
        r".*?(?=item[\s.\xa0]*\d+[\s.\xa0]*[\.:,]?\s*\d+|signature|$)"
    )


def extract_financial_numbers(text: str) -> list[str]:
    patterns = [
        r"\$[\d,]+(?:\.\d{2})?(?:\s*(?:million|billion|trillion|ë°±ë§Œ|ì–µ|ì¡°))?",
        r"[\d,]+(?:\.\d{2})?\s*(?:million|billion|trillion|ë°±ë§Œ|ì–µ|ì¡°)\s*(?:ë‹¬ëŸ¬|dollars?)",
        r"[\d,]+(?:\.\d{2})?\s*(?:shares?|ì£¼ì‹|ì£¼)",
        r"[\d,]+(?:\.\d{2})?\s*(?:percent|%)",
    ]
    out: list[str] = []
    for p in patterns:
        out.extend(re.findall(p, text, flags=re.IGNORECASE))
    return list(set(out))


def safe_soup(raw_html) -> BeautifulSoup:
    """
    ParserRejectedMarkup ë°©ì§€ìš© ì•ˆì „ íŒŒì„œ.
    bytes/str ëª¨ë‘ ì…ë ¥ ê°€ëŠ¥í•˜ë©°,
    - gzip í•´ì œ
    - ì œì–´ë¬¸ì ì œê±°
    - ì¸ì½”ë”© ìë™ ê°ì§€
    - html.parser â†’ lxml â†’ html5lib í´ë°±
    """
    try:
        # â”€â”€â”€â”€â”€ bytes ì²˜ë¦¬ â”€â”€â”€â”€â”€
        if isinstance(raw_html, bytes):
            if raw_html.startswith(b"\x1f\x8b"):
                try:
                    raw_html = gzip.decompress(raw_html)
                except Exception:
                    pass
            cleaned = re.sub(rb"[^\x09\x0A\x0D\x20-\x7E]", b" ", raw_html)
            enc_guess = chardet.detect(cleaned)["encoding"] or "utf-8"
            text = cleaned.decode(enc_guess, errors="replace")
        else:
            # â”€â”€â”€â”€â”€ str ì²˜ë¦¬ â”€â”€â”€â”€â”€
            text = re.sub(r"[^\x09\x0A\x0D\x20-\x7E]", " ", str(raw_html))

        for parser in ("html.parser", "lxml", "html5lib"):
            try:
                return BeautifulSoup(text, parser)
            except Exception:
                continue

        # ë§ˆì§€ë§‰ ìˆ˜ë‹¨
        return BeautifulSoup(UnicodeDammit(text).unicode_markup, "html.parser")
    except Exception as e:
        logger.error("safe_soup ì‹¤íŒ¨: %s", e)
        return BeautifulSoup("", "html.parser")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. ë‚ ì§œ ì¶”ì¶œ ë¡œì§ í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class EnhancedDateExtractor:
    def __init__(self, default_date: str | None = None):
        self.default_date = default_date or date.today().strftime("%Y-%m-%d")
        self.date_patterns = [
            r"(?i)date\s+of\s+report\s*[:\(]?\s*(\w+\s+\d{1,2},?\s+\d{4})",
            r"(?i)date\s+of\s+earliest\s+event\s+reported\s*[:\(]?\s*(\w+\s+\d{1,2},?\s+\d{4})",
            r"(?i)filing\s+date\s*[:\(]?\s*(\w+\s+\d{1,2},?\s+\d{4})",
            r"(\d{4}-\d{2}-\d{2})",
            r"(\d{1,2}/\d{1,2}/\d{4})",
            r"(\d{1,2}-\d{1,2}-\d{4})",
            r"\((\w+\s+\d{1,2},?\s+\d{4})\)",
            r"\((\d{1,2}/\d{1,2}/\d{4})\)",
            r"\((\d{4}-\d{2}-\d{2})\)",
            r"(?i)(?:on|dated?|as\s+of|effective)\s+(\w+\s+\d{1,2},?\s+\d{4})",
            r"(?i)(?:on|dated?|as\s+of|effective)\s+(\d{1,2}/\d{1,2}/\d{4})",
            r"(?i)current\s+report\s+.*?(\w+\s+\d{1,2},?\s+\d{4})",
            r"(?i)form\s+8-k\s+.*?(\w+\s+\d{1,2},?\s+\d{4})",
        ]
        self.date_formats = [
            "%B %d, %Y",
            "%b %d, %Y",
            "%B %d %Y",
            "%b %d %Y",
            "%Y-%m-%d",
            "%m/%d/%Y",
            "%m-%d-%Y",
            "%d/%m/%Y",
            "%d-%m-%Y",
        ]

    def extract_date(self, raw_html) -> str:
        soup = safe_soup(raw_html)

        # 1) SEC-HEADER ì˜ì—­ ìš°ì„ 
        header_date = self._from_sec_header(soup)
        if header_date:
            return header_date

        text = soup.get_text(" ")
        if (date_found := self._with_patterns(text[:2000])) :
            return date_found
        if (date_found := self._with_patterns(text)) :
            return date_found
        if (date_found := self._from_tags(soup)) :
            return date_found

        logger.warning("ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜: %s", self.default_date)
        return self.default_date

    # â”€â”€â”€â”€â”€ ë‚´ë¶€ helper â”€â”€â”€â”€â”€
    def _with_patterns(self, txt: str) -> str | None:
        for pat in self.date_patterns:
            for m in re.findall(pat, txt, flags=re.IGNORECASE):
                if (fd := self._format(m)):
                    return fd
        return None

    def _from_sec_header(self, soup: BeautifulSoup) -> str | None:
        patterns = [
            r"<SEC-HEADER>.*?</SEC-HEADER>",
            r"SEC-HEADER.*?(?=<|$)",
            r"CONFORMED\s+PERIOD\s+OF\s+REPORT[:\s]+(\d{8})",
            r"FILED\s+AS\s+OF\s+DATE[:\s]+(\d{8})",
        ]
        text = str(soup)
        for p in patterns:
            for m in re.findall(p, text, flags=re.DOTALL | re.IGNORECASE):
                if isinstance(m, str) and m.isdigit() and len(m) == 8:
                    return self._format(m, "%Y%m%d")
                if (fd := self._with_patterns(m)):
                    return fd
        return None

    def _from_tags(self, soup: BeautifulSoup) -> str | None:
        for tag in soup.find_all(["time", "date", "span", "div", "p"]):
            for attr in ["datetime", "date", "data-date"]:
                if tag.get(attr) and (fd := self._format(tag[attr])):
                    return fd
            if tag.string and (fd := self._format(tag.string.strip())):
                return fd
        return None

    def _format(self, ds: str, known: str | None = None) -> str | None:
        ds = ds.strip()
        try:
            if known:
                return datetime.strptime(ds, known).strftime("%Y-%m-%d")
            if ds.isdigit() and len(ds) == 8:
                return f"{ds[:4]}-{ds[4:6]}-{ds[6:]}"
            for fmt in self.date_formats:
                try:
                    dt = datetime.strptime(ds, fmt)
                    if 1990 <= dt.year <= datetime.now().year + 5:
                        return dt.strftime("%Y-%m-%d")
                except ValueError:
                    continue
        except Exception:
            pass
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. Smart8KExtractor í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Smart8KExtractor:
    def __init__(self, default_date: str | None = None):
        self.date_extractor = EnhancedDateExtractor(default_date)
        # Item íŒ¨í„´ ì •ì˜
        self.item_patterns: dict[str, dict] = {
            "ITEM_1_01": {
                "pattern": build_item_pattern(1, 1),
                "importance": 8,
                "korean": "ì¤‘ìš” ê³„ì•½ ì²´ê²°",
            },
            "ITEM_1_02": {
                "pattern": build_item_pattern(1, 2),
                "importance": 7,
                "korean": "ê³„ì•½ ì¢…ë£Œ",
            },
            "ITEM_2_01": {
                "pattern": build_item_pattern(2, 1),
                "importance": 9,
                "korean": "ìì‚° ì¸ìˆ˜/ë§¤ê°",
            },
            "ITEM_2_02": {
                "pattern": build_item_pattern(2, 2),
                "importance": 8,
                "korean": "ì‹¤ì  ë°œí‘œ",
            },
            "ITEM_3_02": {
                "pattern": build_item_pattern(3, 2),
                "importance": 6,
                "korean": "ì£¼ì‹ ë§¤ê°",
            },
            "ITEM_4_01": {
                "pattern": build_item_pattern(4, 1),
                "importance": 6,
                "korean": "ê°ì‚¬ì¸ ë³€ê²½",
            },
            "ITEM_5_02": {
                "pattern": build_item_pattern(5, 2),
                "importance": 9,
                "korean": "ì„ì› ë³€ê²½",
            },
            "ITEM_7_01": {
                "pattern": build_item_pattern(7, 1),
                "importance": 7,
                "korean": "ê·œì œ ì ˆì°¨",
            },
            "ITEM_8_01": {
                "pattern": build_item_pattern(8, 1),
                "importance": 5,
                "korean": "ê¸°íƒ€ ì¤‘ìš” ì‚¬ê±´",
            },
            "ITEM_9_01": {
                "pattern": build_item_pattern(9, 1),
                "importance": 4,
                "korean": "ì¬ë¬´ì œí‘œ ë° ì „ì‹œë¬¼",
            },
        }

    # â”€â”€â”€â”€â”€ í•µì‹¬ ë©”ì„œë“œ â”€â”€â”€â”€â”€
    def extract_filing_date(self, raw_html) -> str:
        return self.date_extractor.extract_date(raw_html)

    def smart_filter(self, raw_html) -> dict:
        soup = safe_soup(raw_html)
        for t in soup.find_all(["header", "footer", "nav", "script", "style"]):
            t.decompose()
        full_text = soup.get_text(" ")

        # í…Œì´ë¸” / ê°•ì¡° í…ìŠ¤íŠ¸ ì¶”ì¶œ
        tables = [
            tbl.get_text(" ", strip=True)
            for tbl in soup.find_all("table")
            if any(
                kw in tbl.get_text(" ", strip=True).lower()
                for kw in [
                    "merger",
                    "agreement",
                    "officer",
                    "director",
                    "shares",
                    "vote",
                    "financial",
                    "revenue",
                    "earnings",
                    "dividend",
                    "debt",
                ]
            )
        ][:3]

        emphasizes = [
            em.get_text(" ", strip=True)
            for em in soup.find_all(["b", "strong", "em", "u"])
            if len(em.get_text(strip=True)) > 10
        ][:15]

        return {"full_text": full_text, "tables": tables, "emphasized": emphasizes}

    def extract_important(self, text: str) -> str:
        important: list[str] = []

        # 1) Item ë¸”ë¡
        for code, cfg in sorted(
            self.item_patterns.items(), key=lambda x: x[1]["importance"], reverse=True
        ):
            ms = re.findall(cfg["pattern"], text, flags=re.DOTALL)
            if ms:
                blk = ms[0].strip()
                if len(blk) > 80:
                    important.append(f"[{cfg['korean']}]\n{blk[:1500]}")

        # 2) í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¬¸ì¥
        weighted: list[tuple[str, float]] = []
        for sent in re.split(r"[.!?]+", text):
            sent = sent.strip()
            if len(sent) < 40:
                continue
            score = sum(w for kw, w in ALL_KEYWORDS if kw in sent.lower())
            if score >= 1.5:
                weighted.append((sent, score))
        for s, _ in sorted(weighted, key=lambda x: x[1], reverse=True)[:12]:
            important.append(s)

        # 3) ì¬ë¬´ ìˆ˜ì¹˜
        if (nums := extract_financial_numbers(text)):
            important.append("ì£¼ìš” ìˆ˜ì¹˜: " + ", ".join(nums[:10]))

        combined = "\n\n".join(important[:15])
        if len(enc.encode(combined)) > 4500:
            combined = combined[:12000]
        return combined


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ê²°ê³¼ ê°„ì†Œí™” í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def simplify(results: list[dict]) -> list[dict]:
    return [
        {
            "title": r.get("title", "ì œëª© ì—†ìŒ"),
            "narrative": r.get("narrative", "ë‚´ìš© ì—†ìŒ"),
            "filing_date": r.get("filing_date", "ë‚ ì§œ ì •ë³´ ì—†ìŒ"),
        }
        for r in results
    ]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. ë©”ì¸ ë¶„ì„ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_8k(
    docs: list[str],
    max_cost: float = 8.0,
    default_date: str | None = None,
) -> list[dict]:
    if not docs:
        return []

    default_date = default_date or date.today().strftime("%Y-%m-%d")
    extractor = Smart8KExtractor(default_date)

    results: list[dict] = []
    total_cost = 0.0

    for idx, raw in enumerate(docs, start=1):
        logger.info("ğŸ” ë¬¸ì„œ %d ë¶„ì„ ì¤‘...", idx)

        # ì˜ˆì‚° ì²´í¬
        est_tokens = len(enc.encode(raw[:15_000])) * 0.4
        est_dollars = est_tokens / 1000 * 0.00075
        if total_cost + est_dollars > max_cost:
            logger.warning("ì˜ˆì‚° ì´ˆê³¼ ì˜ˆìƒ, ë¬¸ì„œ %d ìŠ¤í‚µ", idx)
            continue

        filing_date = extractor.extract_filing_date(raw)
        if not re.match(r"\d{4}-\d{2}-\d{2}", filing_date):
            filing_date = default_date

        filt = extractor.smart_filter(raw)
        important = extractor.extract_important(filt["full_text"])

        if filt["tables"]:
            important += f"\n\n[í…Œì´ë¸”]\n{'\n'.join(filt['tables'])[:1500]}"
        if filt["emphasized"]:
            important += f"\n\n[ê°•ì¡°]\n{'\n'.join(filt['emphasized'])}"

        # Rate limit íœ´ì‹
        if idx > 1:
            time.sleep(8)

        # OpenAI í˜¸ì¶œ (ì¬ì‹œë„)
        attempt, response = 0, None
        while attempt < 5 and response is None:
            try:
                response = (
                    PROMPT | llm | StrOutputParser()
                ).invoke({"filing_date": filing_date, "full_content": important})
            except Exception as e:
                if "429" in str(e):
                    delay = 2 * (2**attempt)
                    logger.warning("429 ì¬ì‹œë„ %díšŒì°¨, %ds í›„ ì¬ì‹œë„", attempt + 1, delay)
                    time.sleep(delay)
                    attempt += 1
                else:
                    logger.error("OpenAI ì˜¤ë¥˜: %s", e)
                    break

        if response is None:
            continue

        # JSON íŒŒì‹± ë° ë©”íƒ€ ì¶”ê°€
        try:
            rec = json.loads(response)
        except json.JSONDecodeError:
            logger.error("JSON íŒŒì‹± ì‹¤íŒ¨, ë¬¸ì„œ %d ìŠ¤í‚µ", idx)
            continue

        if not re.match(r"\d{4}-\d{2}-\d{2}", rec.get("filing_date", "")):
            rec["filing_date"] = filing_date

        token_in = len(enc.encode(important))
        token_out = len(enc.encode(response))
        cost = token_in * 0.00015 / 1000 + token_out * 0.00060 / 1000
        total_cost += cost

        rec.update(
            {
                "document_index": idx,
                "processed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "analysis_version": "v4_safe",
                "_meta": {"cost_usd": round(cost, 6)},
            }
        )

        results.append(rec)
        logger.info("âœ… ë¬¸ì„œ %d ì™„ë£Œ (ë¹„ìš© $%.4f)", idx, cost)

        if total_cost >= max_cost:
            logger.info("ğŸ’° ì˜ˆì‚° ì†Œì§„, ë£¨í”„ ì¢…ë£Œ")
            break

    if not results:
        return [
            {
                "title": "ë¶„ì„ ê°€ëŠ¥í•œ ê³µì‹œ ì—†ìŒ",
                "narrative": "íˆ¬ì ê°€ì¹˜ê°€ ë†’ì€ ì •ë³´ë¥¼ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "filing_date": default_date,
            }
        ]

    return simplify(results)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. í…ŒìŠ¤íŠ¸ ì½”ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    sample_docs = ["<html><body><p>Sample 8-K content</p></body></html>"]
    res = analyze_8k(sample_docs)
    print(json.dumps(res, ensure_ascii=False, indent=2))
